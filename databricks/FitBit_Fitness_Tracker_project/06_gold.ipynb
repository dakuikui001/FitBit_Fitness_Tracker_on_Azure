{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8149f378-6a12-4756-8070-737493623434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./01_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c911932-68d9-420b-a9d1-835adb573ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8b48483-84be-4bb3-8787-b7837c8586dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "class Gold():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.landing_zone = self.Conf.landing + 'landing_zone'\n",
    "        self.checkpoint_base = self.Conf.checkpoint + 'checkpoints'\n",
    "        self.initial = self.Conf.medallion + \"initial\"\n",
    "        self.bronze = self.Conf.medallion + \"bronze\"\n",
    "        self.silver = self.Conf.medallion + \"silver\"\n",
    "        self.gold = self.Conf.medallion + \"gold\"\n",
    "        self.catalog = f\"fitbit_{env}_catalog\"\n",
    "        self.db_name = self.Conf.db_name\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "    \n",
    "    def _get_user_date_grid(self):\n",
    "        \"\"\"生成用户与日期的完整笛卡尔积网格\"\"\"\n",
    "        df_date = spark.read.table(f\"{self.catalog}.{self.db_name}.date_list\")\n",
    "        df_user = spark.read.table(f\"{self.catalog}.{self.db_name}.user_list\")\n",
    "        # 使用广播连接优化小表交叉连接性能\n",
    "        return df_user.crossJoin(broadcast(df_date))\n",
    "    \n",
    "    def upsert_activity_daily_gold(self, once=True, processing_time=\"1 minute\"):\n",
    "        \"\"\"以卡路里流为触发源，关联其他维度补全 Gold 表\"\"\"\n",
    "        \n",
    "        # 1. 定义多维度合并的 MERGE SQL\n",
    "        # 涵盖：卡路里、步数、睡眠时长、以及四种活跃分钟数\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.activity_daily_gold a\n",
    "            USING activity_daily_gold_delta b\n",
    "            ON a.user_id = b.user_id AND a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        # 2. 定义内部的微批次处理函数 (foreachBatch)\n",
    "        def process_micro_batch(df_batch, batch_id):\n",
    "            if df_batch.isEmpty():\n",
    "                return\n",
    "\n",
    "            # 获取静态网格\n",
    "            df_grid = self._get_user_date_grid()\n",
    "\n",
    "            # 关联其他 Silver 层表（这些表在微批次执行时作为静态表读取当前快照）\n",
    "            df_daily_steps = spark.read.table(f\"{self.catalog}.{self.db_name}.steps_daily_sl\")\n",
    "            df_sleep = spark.read.table(f\"{self.catalog}.{self.db_name}.sleep_daily_sl\")\n",
    "            df_daily_sleep = df_sleep.groupBy(\"user_id\", \"date\").agg(F.sum(\"asleep_minutes\").alias(\"asleep_minutes\"), F.sum(\"total_minutes_in_bed\").alias(\"total_minutes_in_bed\"))\n",
    "            df_daily_intensities = spark.read.table(f\"{self.catalog}.{self.db_name}.intensities_daily_sl\")\n",
    "            df_daily_heartrate = spark.read.table(f\"{self.catalog}.{self.db_name}.heartrate_daily_sl\")\n",
    "\n",
    "            # 以当前批次的 user/date 为基准，关联所有维度\n",
    "            df_enriched = (df_batch.alias(\"cal\")\n",
    "                .join(df_grid, [\"user_id\", \"date\"], \"inner\") # 确保符合网格规范\n",
    "                .join(df_daily_steps.alias(\"st\"), [\"user_id\", \"date\"], \"left\")\n",
    "                .join(df_daily_sleep.alias(\"sl\"), [\"user_id\", \"date\"], \"left\")\n",
    "                .join(df_daily_intensities.alias(\"it\"), [\"user_id\", \"date\"], \"left\")\n",
    "                .join(df_daily_heartrate.alias(\"hr\"), [\"user_id\", \"date\"], \"left\")\n",
    "                .select(\n",
    "                    \"user_id\", \"date\",\n",
    "                    F.coalesce(F.col(\"st.total_steps\"), F.lit(0)).alias(\"total_steps\"),\n",
    "                    F.coalesce(F.col(\"cal.daily_calories\"), F.lit(0)).alias(\"total_calories\"),\n",
    "                    F.coalesce(F.col(\"it.very_active_minutes\"), F.lit(0)).alias(\"very_active_minutes\"),\n",
    "                    F.coalesce(F.col(\"it.fairly_active_minutes\"), F.lit(0)).alias(\"fairly_active_minutes\"),\n",
    "                    F.coalesce(F.col(\"it.lightly_active_minutes\"), F.lit(0)).alias(\"lightly_active_minutes\"),\n",
    "                    F.coalesce(F.col(\"it.sedentary_minutes\"), F.lit(0)).alias(\"sedentary_minutes\"),\n",
    "                    F.coalesce(F.col(\"hr.avg_heartrate\"), F.lit(0)).alias(\"avg_heartrate\"),\n",
    "                    F.coalesce(F.col(\"hr.max_heartrate\"), F.lit(0)).alias(\"max_heartrate\"),\n",
    "                    F.coalesce(F.col(\"sl.asleep_minutes\"), F.lit(0)).alias(\"asleep_minutes\"),\n",
    "                    F.coalesce(F.col(\"sl.total_minutes_in_bed\"), F.lit(0)).alias(\"total_minutes_in_bed\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 写入临时视图并执行 MERGE\n",
    "            df_enriched.createOrReplaceTempView(\"activity_daily_gold_delta\")\n",
    "            df_batch.sparkSession.sql(query)\n",
    "\n",
    "        # 3. 启动流：以卡路里表作为驱动源\n",
    "        # 只要卡路里有更新，就重新计算该用户该天的所有维度快照\n",
    "        df_stream = (spark.readStream\n",
    "            .option(\"ignoreDeletes\", \"true\")\n",
    "            .table(f\"{self.catalog}.{self.db_name}.calories_daily_sl\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (df_stream.writeStream\n",
    "            .foreachBatch(process_micro_batch)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/activity_daily_gold\")\n",
    "            .queryName(\"activity_daily_gold_stream\")\n",
    "        )\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "  \n",
    "    def _await_queries(self, once):\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "    \n",
    "    def upsert(self, once=True, processing_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting gold layer upsert ...\")\n",
    "        self.upsert_activity_daily_gold(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed gold layer  upsert {int(time.time()) - start} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
