{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6778c97e-d11d-4ac0-b79c-1a60711ec5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./01_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5400df5-8a48-400a-927d-3a6eceebd797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view_name):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view_name = temp_view_name\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view_name)\n",
    "        df_micro_batch._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0101dd-046d-4055-b5ef-e3c70733adb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver():\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.landing_zone = self.Conf.landing + 'landing_zone'\n",
    "        self.checkpoint_base = self.Conf.checkpoint + 'checkpoints'\n",
    "        self.initial = self.Conf.medallion + \"initial\"\n",
    "        self.bronze = self.Conf.medallion + \"bronze\"\n",
    "        self.silver = self.Conf.medallion + \"silver\"\n",
    "        self.gold = self.Conf.medallion + \"gold\"\n",
    "        self.catalog = f\"fitbit_{env}_catalog\"\n",
    "        self.db_name = self.Conf.db_name\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    def upsert_calories_daily_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.calories_daily_sl a\n",
    "            USING calories_daily_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"calories_daily_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.calories_min_bz\")\n",
    "            .selectExpr(\"user_id\", \"activity_minute\", \"calories\", \"date\")\n",
    "            .withWatermark(\"activity_minute\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"activity_minute\"])\n",
    "            .groupBy(\"user_id\", \"date\")\n",
    "            .agg(F.sum(\"calories\").alias(\"daily_calories\"))\n",
    "            .select(\"user_id\", \"daily_calories\", \"date\")\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"calories_daily_sl\", \"calories_daily_sl_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    def upsert_heartrate_min_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.heartrate_min_sl a\n",
    "            USING heartrate_min_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.activity_minute = b.activity_minute\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"heartrate_min_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.heartrate_sec_bz\")\n",
    "            .selectExpr(\"user_id\", \"time\", \"value\", \"date\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"time\"])\n",
    "            .withColumn(\"activity_minute\", F.date_trunc(\"minute\", F.col(\"time\")))\n",
    "            .select(\"user_id\", \"activity_minute\", \"value\", \"date\")\n",
    "            .groupBy(\"user_id\", \"activity_minute\", \"date\")\n",
    "            .agg(F.avg(\"value\").alias(\"avg_heartrate\"),\n",
    "                 F.max(\"value\").alias(\"max_heartrate\"))\n",
    "            .withColumn(\"timeKey\", F.date_format(\"activity_minute\", 'HH:mm:ss'))\n",
    "            .select(\"user_id\", \"activity_minute\", \"avg_heartrate\", \"max_heartrate\", \"date\", \"timeKey\")\n",
    "        )\n",
    "\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"heartrate_min_sl\", \"heartrate_min_sl_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    def upsert_heartrate_daily_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.heartrate_daily_sl a\n",
    "            USING heartrate_daily_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"heartrate_daily_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.heartrate_min_sl\")\n",
    "            .selectExpr(\"user_id\", \"activity_minute\", \"avg_heartrate\", \"max_heartrate\", \"date\")\n",
    "            .withWatermark(\"activity_minute\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"activity_minute\"])\n",
    "            .groupBy(\"user_id\", \"date\")\n",
    "            .agg(F.avg(\"avg_heartrate\").alias(\"avg_heartrate\"),\n",
    "                 F.max(\"max_heartrate\").alias(\"max_heartrate\"))\n",
    "            .select(\"user_id\", \"avg_heartrate\", \"max_heartrate\", \"date\")\n",
    "        )\n",
    "\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"heartrate_daily_sl\", \"heartrate_daily_sl_upsert_stream\", \"silver_p2\", once, processing_time)\n",
    "    \n",
    "    def upsert_intensities_daily_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.intensities_daily_sl a\n",
    "            USING intensities_daily_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"intensities_daily_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.intensities_min_bz\")\n",
    "            .selectExpr(\"user_id\", \"activity_minute\", \"intensity\", \"date\")\n",
    "            .withWatermark(\"activity_minute\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"activity_minute\"])\n",
    "            .groupBy(\"user_id\", \"date\")\n",
    "            .agg(F.sum(when(F.col(\"intensity\")==0, 1).otherwise(0)).alias(\"sedentary_minutes\"),\n",
    "                 F.sum(when(F.col(\"intensity\")==1, 1).otherwise(0)).alias(\"lightly_active_minutes\"),\n",
    "                 F.sum(when(F.col(\"intensity\")==2, 1).otherwise(0)).alias(\"fairly_active_minutes\"),\n",
    "                 F.sum(when(F.col(\"intensity\")==3, 1).otherwise(0)).alias(\"very_active_minutes\"))\n",
    "            .select(\"user_id\", \"sedentary_minutes\", \"lightly_active_minutes\", \"fairly_active_minutes\", \"very_active_minutes\", \"date\")\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"intensities_daily_sl\", \"intensities_daily_sl_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    def upsert_sleep_daily_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.sleep_daily_sl a\n",
    "            USING sleep_daily_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"sleep_daily_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.sleep_min_bz\")\n",
    "            .selectExpr(\"user_id\", \"activity_minute\", \"value\",\"log_id\", \"date\")\n",
    "            .withWatermark(\"activity_minute\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"activity_minute\"])\n",
    "            .groupBy(\"user_id\", \"date\", \"log_id\")\n",
    "            .agg(F.sum(when(F.col(\"value\")==1, 1).otherwise(0)).alias(\"asleep_minutes\"),\n",
    "                 F.sum(when(F.col(\"value\")==2, 1).otherwise(0)).alias(\"Restless_minuts\"),\n",
    "                 F.sum(when(F.col(\"value\")==3, 1).otherwise(0)).alias(\"awake_minutes\"))\n",
    "            .withColumn(\"total_minutes_in_bed\",F.col(\"asleep_minutes\") + F.col(\"Restless_minuts\") + F.col(\"awake_minutes\"))\n",
    "            .select(\"user_id\", \"total_minutes_in_bed\",\"asleep_minutes\",\"Restless_minuts\",\"awake_minutes\", \"log_id\", \"date\")\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"sleep_daily_sl\", \"sleep_daily_sl_upsert_stream\", \"silver_p1\", once, processing_time)\n",
    "    \n",
    "    def upsert_steps_daily_sl(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.steps_daily_sl a\n",
    "            USING steps_daily_sl_delta b\n",
    "            ON a.user_id = b.user_id and a.date = b.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"steps_daily_sl_delta\")\n",
    "\n",
    "        df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.steps_min_bz\")\n",
    "            .selectExpr(\"user_id\", \"activity_minute\", \"steps\", \"date\")\n",
    "            .withWatermark(\"activity_minute\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"activity_minute\"])\n",
    "            .groupBy(\"user_id\", \"date\")\n",
    "            .agg(F.sum(\"steps\").alias(\"total_steps\"))\n",
    "            .select(\"user_id\", \"total_steps\", \"date\")\n",
    "        )\n",
    "        return self._write_stream_update(df_delta, data_upserter, \"steps_daily_sl\", \"steps_daily_sl_upsert_stream\", \"silver_p1\", once, processing_time) \n",
    "    \n",
    "    def upsert_user_list(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "    \n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_list a\n",
    "            USING user_list_delta b\n",
    "            ON a.user_id = b.user_id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"user_list_delta\")\n",
    "\n",
    "        source_tables = [\n",
    "            \"calories_min_bz\",\n",
    "            \"heartrate_sec_bz\",\n",
    "            \"intensities_min_bz\",\n",
    "            \"mets_min_bz\",\n",
    "            \"sleep_min_bz\",\n",
    "            \"steps_min_bz\",\n",
    "            \"weight_daily_bz\"\n",
    "        ]\n",
    "\n",
    "        # 3. 遍历表并进行 Union\n",
    "        df_union = None\n",
    "        for table_name in source_tables:\n",
    "            temp_df = (spark.readStream\n",
    "                .option(\"startingVersion\", startingVersion)\n",
    "                .option(\"ignoreDeletes\", True)\n",
    "                .table(f\"{self.catalog}.{self.db_name}.{table_name}\")\n",
    "                .select(\"user_id\") # 只取 ID 提高性能\n",
    "            )\n",
    "            \n",
    "            if df_union is None:\n",
    "                df_union = temp_df\n",
    "            else:\n",
    "                df_union = df_union.union(temp_df)\n",
    "\n",
    "        # 4. 对合并后的流进行去重\n",
    "        df_final = df_union.dropDuplicates([\"user_id\"])\n",
    "\n",
    "        # 5. 写入流\n",
    "        return self._write_stream_update(df_final, data_upserter, \"user_list\",\"user_list_upsert_stream\", \"silver_p3\", once, processing_time)\n",
    "    \n",
    "    def upsert_date_list(self, once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "        from pyspark.sql import functions as F\n",
    "    \n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.date_list a\n",
    "            USING date_list_delta b\n",
    "            ON a.date = b.date\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        data_upserter = Upserter(query, \"date_list_delta\")\n",
    "\n",
    "        source_tables = [\n",
    "            \"calories_min_bz\",\n",
    "            \"heartrate_sec_bz\",\n",
    "            \"intensities_min_bz\",\n",
    "            \"mets_min_bz\",\n",
    "            \"sleep_min_bz\",\n",
    "            \"steps_min_bz\",\n",
    "            \"weight_daily_bz\"\n",
    "        ]\n",
    "\n",
    "        # 3. 遍历表并进行 Union\n",
    "        df_union = None\n",
    "        for table_name in source_tables:\n",
    "            temp_df = (spark.readStream\n",
    "                .option(\"startingVersion\", startingVersion)\n",
    "                .option(\"ignoreDeletes\", True)\n",
    "                .table(f\"{self.catalog}.{self.db_name}.{table_name}\")\n",
    "                .select(\"date\") # 只取 ID 提高性能\n",
    "            )\n",
    "            \n",
    "            if df_union is None:\n",
    "                df_union = temp_df\n",
    "            else:\n",
    "                df_union = df_union.union(temp_df)\n",
    "\n",
    "        # 4. 对合并后的流进行去重\n",
    "        df_final = df_union.dropDuplicates([\"date\"])\n",
    "\n",
    "        # 5. 写入流\n",
    "        return self._write_stream_update(df_final, data_upserter, \"date_list\",\"date_list_upsert_stream\", \"silver_p3\", once, processing_time)\n",
    "  \n",
    "    def _write_stream_update(self, df, upserter, path, query_name, pool, once, processing_time):\n",
    "        stream_writer = (df.writeStream\n",
    "            .foreachBatch(upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/{path}\")\n",
    "            .queryName(query_name)\n",
    "        )\n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", pool)\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "    def _await_queries(self, once):\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "    \n",
    "    def upsert(self, once=True, processing_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting silver layer upsert ...\")\n",
    "\n",
    "        self.upsert_calories_daily_sl(once, processing_time)\n",
    "        self.upsert_heartrate_min_sl(once, processing_time)\n",
    "        self.upsert_intensities_daily_sl(once, processing_time)\n",
    "        self.upsert_sleep_daily_sl(once, processing_time)\n",
    "        self.upsert_steps_daily_sl(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 1 upsert {int(time.time()) - start} seconds\")\n",
    "    \n",
    "        self.upsert_heartrate_daily_sl(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 2 upsert {int(time.time()) - start} seconds\")\n",
    "\n",
    "        self.upsert_user_list(once, processing_time)\n",
    "        self.upsert_date_list(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 3 upsert {int(time.time()) - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
